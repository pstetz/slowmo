{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from os.path import basename, dirname, isdir, isfile, join\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Volumes/hd_4tb/results/training/*/*\"\n",
    "dfs = list()\n",
    "files = sorted(glob(root))\n",
    "for folder in tqdm(files):\n",
    "    df = pd.read_csv(join(folder, \"norm_simple.csv\"))\n",
    "    df[\"pred\"] = np.load(join(folder, \"pred.npy\"))\n",
    "    dfs.append(df)\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "df.reset_index(drop=True).to_feather(\"/Volumes/hd_4tb/results/summary/norm_all.feather\")\n",
    "\n",
    "files = glob(\"/Volumes/hd_4tb/results/training/0110/*/norm_simple.csv\")\n",
    "dfs = list()\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    df[\"pred\"] = np.load(join(dirname(f), \"pred.npy\"))\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "train_y = df.pop(\"pred\")\n",
    "preds = lgbm_model.predict(df)\n",
    "guess = (df[\"next\"] + df[\"prev\"]) / 2\n",
    "np.mean(np.abs(train_y - preds)), np.mean(np.abs(train_y - guess))\n",
    "np.std(np.abs(train_y - preds)), np.std(np.abs(train_y - guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "<div hidden>\n",
    "# root = \"/Volumes/hd_4tb/results/training/*/*\"\n",
    "# dfs = list()\n",
    "# for folder in tqdm(glob(root)):\n",
    "#     df = pd.read_csv(join(folder, \"simple.csv\"))\n",
    "#     df[\"pred\"] = np.load(join(folder, \"pred.npy\"))\n",
    "#     dfs.append(df)\n",
    "    \n",
    "# df = pd.concat(dfs)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_feather(\"/Users/pstetz/Desktop/confidential/.project/summary/norm_all.feather\")\n",
    "# df = df.sample(frac=1)\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if \"next\" not in c]\n",
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x = train_test_split(df, random_state=5, train_size=0.80)\n",
    "train_y = train_x.pop(\"pred\")\n",
    "test_y = test_x.pop(\"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=60,\n",
       "              min_split_gain=0.25, n_estimators=125, n_jobs=-1, num_leaves=31,\n",
       "              objective='mean_absolute_error', random_state=None, reg_alpha=0.0,\n",
       "              reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "              subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "#     \"num_iteration\": [3000, 4000, 4500],\n",
    "#     'max_depth': [5, 7, 15], \n",
    "#     'reg_alpha': [0, 0.1], 'reg_lambda': [0.1, 1], \n",
    "    \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"min_split_gain\": [0.2, 0.25], #\"min_data_in_leaf\": [60], #'min_child_weight': 4,\n",
    "#     \"n_estimators\": [125] #, \"feature_fraction\": 0.5\n",
    "}\n",
    "# mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "model = lgb.LGBMRegressor(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"mean_absolute_error\",\n",
    "    learning_rate=0.1,\n",
    "    min_data_in_leaf=60,\n",
    "    min_split_gain=0.25,\n",
    "    n_estimators=125,\n",
    "#     num_leaves=85,\n",
    "    max_depth=-1,\n",
    ")\n",
    "# gs = GridSearchCV(model, param_grid=params, cv=5, verbose=10, scoring=mae_scorer)\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([16.28457785, 21.97705793, 29.57167625, 30.89967799, 20.19947004]),\n",
       " 'score_time': array([0.59632802, 0.50218415, 0.52898574, 0.85004783, 0.46883893]),\n",
       " 'test_score': array([-0.69077921, -0.68144191, -0.68156026, -0.68215386, -0.67898879])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "model = lgb.LGBMRegressor(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"mse\",\n",
    "    learning_rate=0.05,\n",
    "    min_data_in_leaf=80,\n",
    "    min_split_gain=0.3,\n",
    "    n_estimators=130,\n",
    "    num_leaves=90,\n",
    "    max_depth=-1,\n",
    ")\n",
    "\n",
    "cross_validate(model, train_x, train_y, cv=5, scoring=mse_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = \"/Users/pstetz/Desktop/confidential/.project/run/lgbm/4_mae.pkl\"\n",
    "\n",
    "if not isdir(dirname(model_filepath)):\n",
    "    os.makedirs(dirname(model_filepath))\n",
    "    \n",
    "with open(model_filepath, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
