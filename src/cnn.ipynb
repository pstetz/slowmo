{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "from os.path import join, isfile\n",
    "from random import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tf.random.set_seed(5)\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "root = \"/Users/pstetz/Desktop/confidential/.project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "<div hidden>\n",
    "def _load_model():\n",
    "    lr = LeakyReLU(alpha=0.05); lr.__name__ = 'leaky_relu'\n",
    "    relu = ReLU(); relu.__name__ = \"relu\"\n",
    "    activation = relu\n",
    "    use_bias = False\n",
    "    def layer_a(dim):\n",
    "        return layers.Dense(dim, activation=activation, use_bias=use_bias)\n",
    "    \n",
    "    info_input = keras.Input(shape=(130,), name=\"info\")\n",
    "    prev_input = keras.Input(shape=(9, 9, 9, 2), name=\"prev\")\n",
    "    next_input = keras.Input(shape=(9, 9, 9, 2), name=\"next\")\n",
    "    prev_s, next_s = [prev_input], [next_input]\n",
    "    for i, layer in enumerate([\n",
    "        layers.Conv3D(8, (2, 2, 2), use_bias=False),\n",
    "        layers.MaxPool3D(),\n",
    "        layers.Conv3D(8, (2, 2, 2), use_bias=False),\n",
    "        layers.Conv3D(8, (2, 2, 2), use_bias=False),\n",
    "        layers.Flatten()\n",
    "    ]):\n",
    "        prev_s.append(layer(prev_s[-1]))\n",
    "        next_s.append(layer(next_s[-1]))\n",
    "    info_s = [info_input]\n",
    "#     for dim in (130, 130, 130):\n",
    "#         info_s.append(layer_a(dim)(info_s[-1]))\n",
    "\n",
    "    x_0 = layers.concatenate([prev_s[-1], next_s[-1], info_s[-1]])\n",
    "    x_s = [x_0]\n",
    "#     for dim in (146, 146, 146, 146, 128, 64, 32):\n",
    "    for dim in (258, 258, 128, 128, 64, 32):\n",
    "        x_s.append(layer_a(dim)(x_s[-1]))\n",
    "    \n",
    "    bold_signal = layers.Dense(1, name=\"bold_signal\")(x_s[-1])\n",
    "    model = keras.Model(inputs=[prev_input, next_input, info_input], outputs=[bold_signal])\n",
    "    learning_rate = 1e-4\n",
    "    model.compile(optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=8e-2, decay=learning_rate/30),\n",
    "      loss={\"bold_signal\": \"mse\"},\n",
    "      loss_weights=[1.])\n",
    "    return model\n",
    "    \n",
    "def _load_model(with_lgbm = False):\n",
    "    lr = LeakyReLU(alpha=0.05); lr.__name__ = 'leaky_relu'\n",
    "    relu = ReLU(); relu.__name__ = \"relu\"\n",
    "#     activation = lr\n",
    "    \n",
    "    info_input = keras.Input(shape=(130 + int(with_lgbm),), name=\"info\")\n",
    "    prev_input = keras.Input(shape=(9, 9, 9, 2), name=\"prev\")\n",
    "    next_input = keras.Input(shape=(9, 9, 9, 2), name=\"next\")\n",
    "    prev_s, next_s = [prev_input], [next_input]\n",
    "    for i, layer in enumerate([\n",
    "        layers.Conv3D(32, (2, 2, 2), use_bias=False, activation=\"elu\"),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Conv3D(32, (2, 2, 2), use_bias=False, activation=\"elu\"),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Flatten()\n",
    "    ]):\n",
    "        prev_s.append(layer(prev_s[-1]))\n",
    "        next_s.append(layer(next_s[-1]))\n",
    "    info_s = [info_input]\n",
    "    x_0 = layers.concatenate([prev_s[-1], next_s[-1], info_s[-1]])\n",
    "    x_s = [x_0]\n",
    "    init_shape = x_0.shape[1]\n",
    "    for layer in (\n",
    "        layers.Reshape((init_shape, 1)),\n",
    "        layers.Conv1D(256, 256, strides=init_shape, activation='elu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Reshape((256, 1)),\n",
    "        layers.Dropout(0.15),\n",
    "        layers.Conv1D(256, 256, strides=init_shape, activation='elu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Reshape((256, 1)),\n",
    "        layers.Dropout(0.15),\n",
    "        layers.Conv1D(128, 256, activation='elu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Reshape((128, 1)),\n",
    "        layers.Conv1D(64, 128, activation='elu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Reshape((64, 1)),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Conv1D(32, 64, activation='elu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Reshape((32, 1)),\n",
    "        layers.AveragePooling1D(2),\n",
    "        layers.Flatten(),\n",
    "    ):\n",
    "        x_s.append(layer(x_s[-1]))\n",
    "    \n",
    "    bold_signal = layers.Dense(1, name=\"bold_signal\")(x_s[-1])\n",
    "    model = keras.Model(inputs=[prev_input, next_input, info_input], outputs=[bold_signal])\n",
    "    learning_rate = 1e-3\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=8e-1, decay=learning_rate/30),\n",
    "        loss={\"bold_signal\": \"mse\"}, loss_weights=[1.]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "if \"model\" in locals(): del model\n",
    "model = _load_model()\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 258)\n"
     ]
    }
   ],
   "source": [
    "def _load_model():\n",
    "    lr = LeakyReLU(alpha=0.05); lr.__name__ = 'leaky_relu'\n",
    "    relu = ReLU(); relu.__name__ = \"relu\"\n",
    "    activation = lr\n",
    "    use_bias = False\n",
    "    def layer_a(dim):\n",
    "        return layers.Dense(dim, activation=relu, use_bias=use_bias, kernel_constraint=max_norm(3))\n",
    "    \n",
    "    info_input = keras.Input(shape=(130,), name=\"info\")\n",
    "    prev_input = keras.Input(shape=(9, 9, 9, 2), name=\"prev\")\n",
    "    next_input = keras.Input(shape=(9, 9, 9, 2), name=\"next\")\n",
    "    prev_s, next_s = [prev_input], [next_input]\n",
    "    for i, layer in enumerate([\n",
    "        layers.Conv3D(4, (2, 2, 2), use_bias=False, kernel_constraint=max_norm(3)),\n",
    "        layers.Conv3D(4, (2, 2, 2), use_bias=False, kernel_constraint=max_norm(3)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"elu\", kernel_constraint=max_norm(3)),\n",
    "    ]):\n",
    "        prev_s.append(layer(prev_s[-1]))\n",
    "        next_s.append(layer(next_s[-1]))\n",
    "    info_s = [info_input]\n",
    "    x_0 = layers.concatenate([prev_s[-1], next_s[-1], info_s[-1]])\n",
    "    print(x_0.shape)\n",
    "    x_s = [x_0]\n",
    "    for dim in (258, 256, 256, 128, 128): #, 128, 128, 128, 64, 32):\n",
    "        x_s.append(layer_a(dim)(x_s[-1]))\n",
    "#         x_s.append(layers.BatchNormalization()(x_s[-1]))\n",
    "#         x_s.append(layers.Dropout(0.5)(x_s[-1]))\n",
    "    \n",
    "    bold_signal = layers.Dense(1, name=\"bold_signal\")(x_s[-1])\n",
    "    model = keras.Model(inputs=[prev_input, next_input, info_input], outputs=[bold_signal])\n",
    "    learning_rate = 5e-4\n",
    "    model.compile(optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=3e-2, decay=learning_rate/20),\n",
    "      loss={\"bold_signal\": \"mse\"},\n",
    "      loss_weights=[1.])\n",
    "    return model\n",
    "\n",
    "if \"model\" in locals(): del model\n",
    "model = _load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"/Users/pstetz/Desktop/confidential/.project/summary/consolidate\"\n",
    "log_dir = join(root, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "history = History()\n",
    "\n",
    "batch_size = 32\n",
    "num_epoches = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8802c0b7c904869ac5e30d51d6ae39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125952 samples, validate on 31488 samples\n",
      "Epoch 1/3\n",
      "125952/125952 [==============================] - 115s 916us/sample - loss: 0.8166 - val_loss: 0.7247\n",
      "Epoch 2/3\n",
      "125952/125952 [==============================] - 118s 933us/sample - loss: 0.7175 - val_loss: 0.6962\n",
      "Epoch 3/3\n",
      "125952/125952 [==============================] - 107s 846us/sample - loss: 0.6953 - val_loss: 0.6817\n",
      "Train on 130048 samples, validate on 32512 samples\n",
      "Epoch 1/3\n",
      "130048/130048 [==============================] - 111s 857us/sample - loss: 0.6923 - val_loss: 0.7306\n",
      "Epoch 2/3\n",
      "130048/130048 [==============================] - 114s 874us/sample - loss: 0.6808 - val_loss: 0.7246\n",
      "Epoch 3/3\n",
      "130048/130048 [==============================] - 111s 850us/sample - loss: 0.6730 - val_loss: 0.7208\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 118s 925us/sample - loss: 0.7038 - val_loss: 0.6885\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 108s 847us/sample - loss: 0.6979 - val_loss: 0.6856\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 108s 841us/sample - loss: 0.6933 - val_loss: 0.6833\n",
      "Train on 125952 samples, validate on 31488 samples\n",
      "Epoch 1/3\n",
      "125952/125952 [==============================] - 112s 892us/sample - loss: 0.6773 - val_loss: 0.6575\n",
      "Epoch 2/3\n",
      "125952/125952 [==============================] - 108s 860us/sample - loss: 0.6734 - val_loss: 0.6562\n",
      "Epoch 3/3\n",
      "125952/125952 [==============================] - 118s 938us/sample - loss: 0.6702 - val_loss: 0.6537\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 116s 949us/sample - loss: 0.6933 - val_loss: 0.6956\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 114s 939us/sample - loss: 0.6904 - val_loss: 0.6938\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 120s 987us/sample - loss: 0.6878 - val_loss: 0.6928\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 111s 911us/sample - loss: 0.6541 - val_loss: 0.7026\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 114s 934us/sample - loss: 0.6513 - val_loss: 0.7022\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 112s 922us/sample - loss: 0.6491 - val_loss: 0.7014\n",
      "Train on 124928 samples, validate on 31232 samples\n",
      "Epoch 1/3\n",
      "124928/124928 [==============================] - 116s 926us/sample - loss: 0.6548 - val_loss: 0.6824\n",
      "Epoch 2/3\n",
      "124928/124928 [==============================] - 117s 934us/sample - loss: 0.6524 - val_loss: 0.6820\n",
      "Epoch 3/3\n",
      "124928/124928 [==============================] - 121s 969us/sample - loss: 0.6504 - val_loss: 0.6817\n",
      "Train on 126976 samples, validate on 31744 samples\n",
      "Epoch 1/3\n",
      "126976/126976 [==============================] - 117s 921us/sample - loss: 0.6515 - val_loss: 0.6697\n",
      "Epoch 2/3\n",
      "126976/126976 [==============================] - 119s 939us/sample - loss: 0.6493 - val_loss: 0.6692\n",
      "Epoch 3/3\n",
      "126976/126976 [==============================] - 117s 920us/sample - loss: 0.6474 - val_loss: 0.6685\n",
      "Train on 126976 samples, validate on 31744 samples\n",
      "Epoch 1/3\n",
      "126976/126976 [==============================] - 89s 700us/sample - loss: 0.6525 - val_loss: 0.6276\n",
      "Epoch 2/3\n",
      "126976/126976 [==============================] - 61s 477us/sample - loss: 0.6508 - val_loss: 0.6269\n",
      "Epoch 3/3\n",
      "126976/126976 [==============================] - 62s 492us/sample - loss: 0.6494 - val_loss: 0.6264\n",
      "Train on 124928 samples, validate on 31232 samples\n",
      "Epoch 1/3\n",
      "124928/124928 [==============================] - 61s 489us/sample - loss: 0.6901 - val_loss: 0.6734\n",
      "Epoch 2/3\n",
      "124928/124928 [==============================] - 64s 509us/sample - loss: 0.6885 - val_loss: 0.6730\n",
      "Epoch 3/3\n",
      "124928/124928 [==============================] - 62s 498us/sample - loss: 0.6872 - val_loss: 0.6729\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 66s 513us/sample - loss: 0.6617 - val_loss: 0.5821\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 63s 492us/sample - loss: 0.6604 - val_loss: 0.5817\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 63s 492us/sample - loss: 0.6592 - val_loss: 0.5816\n",
      "Train on 123904 samples, validate on 30976 samples\n",
      "Epoch 1/3\n",
      "123904/123904 [==============================] - 64s 513us/sample - loss: 0.6425 - val_loss: 0.6726\n",
      "Epoch 2/3\n",
      "123904/123904 [==============================] - 63s 508us/sample - loss: 0.6411 - val_loss: 0.6728\n",
      "Epoch 3/3\n",
      "123904/123904 [==============================] - 62s 502us/sample - loss: 0.6401 - val_loss: 0.6723\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d11b26b21d4741a6dbf66d21970811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125952 samples, validate on 31488 samples\n",
      "Epoch 1/3\n",
      "125952/125952 [==============================] - 62s 493us/sample - loss: 0.6443 - val_loss: 0.6329\n",
      "Epoch 2/3\n",
      "125952/125952 [==============================] - 63s 504us/sample - loss: 0.6430 - val_loss: 0.6328\n",
      "Epoch 3/3\n",
      "125952/125952 [==============================] - 63s 501us/sample - loss: 0.6421 - val_loss: 0.6326\n",
      "Train on 130048 samples, validate on 32512 samples\n",
      "Epoch 1/3\n",
      "130048/130048 [==============================] - 62s 474us/sample - loss: 0.6441 - val_loss: 0.6911\n",
      "Epoch 2/3\n",
      "130048/130048 [==============================] - 61s 471us/sample - loss: 0.6430 - val_loss: 0.6909\n",
      "Epoch 3/3\n",
      "130048/130048 [==============================] - 61s 468us/sample - loss: 0.6421 - val_loss: 0.6909\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 65s 509us/sample - loss: 0.6722 - val_loss: 0.6589\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 65s 508us/sample - loss: 0.6713 - val_loss: 0.6588\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 64s 499us/sample - loss: 0.6705 - val_loss: 0.6590\n",
      "Train on 125952 samples, validate on 31488 samples\n",
      "Epoch 1/3\n",
      "125952/125952 [==============================] - 61s 488us/sample - loss: 0.6545 - val_loss: 0.6370\n",
      "Epoch 2/3\n",
      "125952/125952 [==============================] - 63s 503us/sample - loss: 0.6537 - val_loss: 0.6367\n",
      "Epoch 3/3\n",
      "125952/125952 [==============================] - 64s 509us/sample - loss: 0.6529 - val_loss: 0.6364\n",
      "Train on 132096 samples, validate on 33024 samples\n",
      "Epoch 1/3\n",
      "132096/132096 [==============================] - 65s 492us/sample - loss: 0.6725 - val_loss: 0.6888\n",
      "Epoch 2/3\n",
      "132096/132096 [==============================] - 67s 510us/sample - loss: 0.6717 - val_loss: 0.6884\n",
      "Epoch 3/3\n",
      "132096/132096 [==============================] - 65s 492us/sample - loss: 0.6710 - val_loss: 0.6881\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 59s 485us/sample - loss: 0.6752 - val_loss: 0.6787\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 61s 498us/sample - loss: 0.6746 - val_loss: 0.6783\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 61s 504us/sample - loss: 0.6739 - val_loss: 0.6781\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 59s 486us/sample - loss: 0.6377 - val_loss: 0.6906\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 59s 481us/sample - loss: 0.6368 - val_loss: 0.6908\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 58s 477us/sample - loss: 0.6362 - val_loss: 0.6906\n",
      "Train on 125235 samples, validate on 31309 samples\n",
      "Epoch 1/3\n",
      "125235/125235 [==============================] - 76s 611us/sample - loss: 0.6578 - val_loss: 0.6607\n",
      "Epoch 2/3\n",
      "125235/125235 [==============================] - 79s 630us/sample - loss: 0.6571 - val_loss: 0.6605\n",
      "Epoch 3/3\n",
      "125235/125235 [==============================] - 71s 566us/sample - loss: 0.6564 - val_loss: 0.6607\n",
      "Train on 130048 samples, validate on 32512 samples\n",
      "Epoch 1/3\n",
      "130048/130048 [==============================] - 64s 492us/sample - loss: 0.6557 - val_loss: 0.6715\n",
      "Epoch 2/3\n",
      "130048/130048 [==============================] - 65s 501us/sample - loss: 0.6549 - val_loss: 0.6716\n",
      "Epoch 3/3\n",
      "130048/130048 [==============================] - 63s 487us/sample - loss: 0.6543 - val_loss: 0.6712\n",
      "Train on 124928 samples, validate on 31232 samples\n",
      "Epoch 1/3\n",
      "124928/124928 [==============================] - 60s 481us/sample - loss: 0.6403 - val_loss: 0.6683\n",
      "Epoch 2/3\n",
      "124928/124928 [==============================] - 61s 492us/sample - loss: 0.6395 - val_loss: 0.6682\n",
      "Epoch 3/3\n",
      "124928/124928 [==============================] - 62s 498us/sample - loss: 0.6389 - val_loss: 0.6683\n",
      "Train on 126976 samples, validate on 31744 samples\n",
      "Epoch 1/3\n",
      "126976/126976 [==============================] - 61s 481us/sample - loss: 0.6380 - val_loss: 0.6606\n",
      "Epoch 2/3\n",
      "126976/126976 [==============================] - 69s 541us/sample - loss: 0.6373 - val_loss: 0.6605\n",
      "Epoch 3/3\n",
      "126976/126976 [==============================] - 62s 488us/sample - loss: 0.6367 - val_loss: 0.6603\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 60s 493us/sample - loss: 0.6123 - val_loss: 0.6745\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 67s 550us/sample - loss: 0.6117 - val_loss: 0.6746\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 60s 496us/sample - loss: 0.6112 - val_loss: 0.6744\n",
      "Train on 126976 samples, validate on 31744 samples\n",
      "Epoch 1/3\n",
      "126976/126976 [==============================] - 62s 490us/sample - loss: 0.6414 - val_loss: 0.6165\n",
      "Epoch 2/3\n",
      "126976/126976 [==============================] - 68s 537us/sample - loss: 0.6408 - val_loss: 0.6163\n",
      "Epoch 3/3\n",
      "126976/126976 [==============================] - 61s 483us/sample - loss: 0.6403 - val_loss: 0.6162\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 61s 498us/sample - loss: 0.6525 - val_loss: 0.6318\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 61s 497us/sample - loss: 0.6519 - val_loss: 0.6317\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 60s 491us/sample - loss: 0.6514 - val_loss: 0.6317\n",
      "Train on 117760 samples, validate on 29440 samples\n",
      "Epoch 1/3\n",
      "117760/117760 [==============================] - 59s 501us/sample - loss: 0.6710 - val_loss: 0.6712\n",
      "Epoch 2/3\n",
      "117760/117760 [==============================] - 58s 491us/sample - loss: 0.6703 - val_loss: 0.6710\n",
      "Epoch 3/3\n",
      "117760/117760 [==============================] - 58s 494us/sample - loss: 0.6697 - val_loss: 0.6709\n",
      "Train on 124928 samples, validate on 31232 samples\n",
      "Epoch 1/3\n",
      "124928/124928 [==============================] - 60s 481us/sample - loss: 0.6794 - val_loss: 0.6646\n",
      "Epoch 2/3\n",
      "124928/124928 [==============================] - 63s 508us/sample - loss: 0.6788 - val_loss: 0.6644\n",
      "Epoch 3/3\n",
      "124928/124928 [==============================] - 62s 493us/sample - loss: 0.6784 - val_loss: 0.6644\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 64s 499us/sample - loss: 0.6595 - val_loss: 0.6557\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 64s 499us/sample - loss: 0.6588 - val_loss: 0.6557\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 62s 486us/sample - loss: 0.6583 - val_loss: 0.6558\n",
      "Train on 123904 samples, validate on 30976 samples\n",
      "Epoch 1/3\n",
      "123904/123904 [==============================] - 60s 486us/sample - loss: 0.6728 - val_loss: 0.6675\n",
      "Epoch 2/3\n",
      "123904/123904 [==============================] - 63s 506us/sample - loss: 0.6722 - val_loss: 0.6675\n",
      "Epoch 3/3\n",
      "123904/123904 [==============================] - 63s 505us/sample - loss: 0.6717 - val_loss: 0.6675\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 63s 492us/sample - loss: 0.6521 - val_loss: 0.5715\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 63s 494us/sample - loss: 0.6516 - val_loss: 0.5714\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 60s 473us/sample - loss: 0.6512 - val_loss: 0.5716\n",
      "Train on 123904 samples, validate on 30976 samples\n",
      "Epoch 1/3\n",
      "123904/123904 [==============================] - 64s 513us/sample - loss: 0.6333 - val_loss: 0.6643\n",
      "Epoch 2/3\n",
      "123904/123904 [==============================] - 62s 499us/sample - loss: 0.6326 - val_loss: 0.6647\n",
      "Epoch 3/3\n",
      "123904/123904 [==============================] - 64s 515us/sample - loss: 0.6322 - val_loss: 0.6647\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5aa0a5aa0d4c54b138243facdcd4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125952 samples, validate on 31488 samples\n",
      "Epoch 1/3\n",
      "125952/125952 [==============================] - 64s 507us/sample - loss: 0.6369 - val_loss: 0.6264\n",
      "Epoch 2/3\n",
      "125952/125952 [==============================] - 61s 485us/sample - loss: 0.6363 - val_loss: 0.6263\n",
      "Epoch 3/3\n",
      "125952/125952 [==============================] - 62s 496us/sample - loss: 0.6359 - val_loss: 0.6264\n",
      "Train on 130048 samples, validate on 32512 samples\n",
      "Epoch 1/3\n",
      "130048/130048 [==============================] - 67s 515us/sample - loss: 0.6370 - val_loss: 0.6847\n",
      "Epoch 2/3\n",
      "130048/130048 [==============================] - 65s 499us/sample - loss: 0.6366 - val_loss: 0.6847\n",
      "Epoch 3/3\n",
      "130048/130048 [==============================] - 63s 488us/sample - loss: 0.6362 - val_loss: 0.6848\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 63s 493us/sample - loss: 0.6658 - val_loss: 0.6519\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 65s 506us/sample - loss: 0.6654 - val_loss: 0.6519\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 62s 485us/sample - loss: 0.6651 - val_loss: 0.6522\n",
      "Train on 125952 samples, validate on 31488 samples\n",
      "Epoch 1/3\n",
      "125952/125952 [==============================] - 64s 507us/sample - loss: 0.6487 - val_loss: 0.6317\n",
      "Epoch 2/3\n",
      "125952/125952 [==============================] - 63s 497us/sample - loss: 0.6483 - val_loss: 0.6316\n",
      "Epoch 3/3\n",
      "125952/125952 [==============================] - 61s 486us/sample - loss: 0.6480 - val_loss: 0.6315\n",
      "Train on 132096 samples, validate on 33024 samples\n",
      "Epoch 1/3\n",
      "132096/132096 [==============================] - 66s 500us/sample - loss: 0.6666 - val_loss: 0.6844\n",
      "Epoch 2/3\n",
      "132096/132096 [==============================] - 70s 534us/sample - loss: 0.6662 - val_loss: 0.6842\n",
      "Epoch 3/3\n",
      "132096/132096 [==============================] - 67s 507us/sample - loss: 0.6659 - val_loss: 0.6841\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 63s 521us/sample - loss: 0.6707 - val_loss: 0.6738\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 60s 494us/sample - loss: 0.6704 - val_loss: 0.6737\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 60s 496us/sample - loss: 0.6701 - val_loss: 0.6736\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 60s 496us/sample - loss: 0.6328 - val_loss: 0.6865\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 62s 507us/sample - loss: 0.6323 - val_loss: 0.6866\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 62s 508us/sample - loss: 0.6320 - val_loss: 0.6866\n",
      "Train on 125235 samples, validate on 31309 samples\n",
      "Epoch 1/3\n",
      "125235/125235 [==============================] - 76s 605us/sample - loss: 0.6523 - val_loss: 0.6569\n",
      "Epoch 2/3\n",
      "125235/125235 [==============================] - 70s 562us/sample - loss: 0.6520 - val_loss: 0.6568\n",
      "Epoch 3/3\n",
      "125235/125235 [==============================] - 69s 548us/sample - loss: 0.6516 - val_loss: 0.6570\n",
      "Train on 130048 samples, validate on 32512 samples\n",
      "Epoch 1/3\n",
      "130048/130048 [==============================] - 64s 496us/sample - loss: 0.6513 - val_loss: 0.6675\n",
      "Epoch 2/3\n",
      "130048/130048 [==============================] - 64s 491us/sample - loss: 0.6508 - val_loss: 0.6675\n",
      "Epoch 3/3\n",
      "130048/130048 [==============================] - 64s 496us/sample - loss: 0.6504 - val_loss: 0.6674\n",
      "Train on 124928 samples, validate on 31232 samples\n",
      "Epoch 1/3\n",
      "124928/124928 [==============================] - 65s 523us/sample - loss: 0.6364 - val_loss: 0.6639\n",
      "Epoch 2/3\n",
      "124928/124928 [==============================] - 61s 487us/sample - loss: 0.6359 - val_loss: 0.6638\n",
      "Epoch 3/3\n",
      "124928/124928 [==============================] - 62s 500us/sample - loss: 0.6356 - val_loss: 0.6639\n",
      "Train on 126976 samples, validate on 31744 samples\n",
      "Epoch 1/3\n",
      "126976/126976 [==============================] - 64s 507us/sample - loss: 0.6338 - val_loss: 0.6577\n",
      "Epoch 2/3\n",
      "126976/126976 [==============================] - 59s 469us/sample - loss: 0.6335 - val_loss: 0.6577\n",
      "Epoch 3/3\n",
      "126976/126976 [==============================] - 63s 494us/sample - loss: 0.6331 - val_loss: 0.6576\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 61s 504us/sample - loss: 0.6082 - val_loss: 0.6707\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 60s 491us/sample - loss: 0.6079 - val_loss: 0.6709\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 63s 518us/sample - loss: 0.6076 - val_loss: 0.6708\n",
      "Train on 126976 samples, validate on 31744 samples\n",
      "Epoch 1/3\n",
      "126976/126976 [==============================] - 76s 602us/sample - loss: 0.6380 - val_loss: 0.6124\n",
      "Epoch 2/3\n",
      "126976/126976 [==============================] - 71s 562us/sample - loss: 0.6377 - val_loss: 0.6123\n",
      "Epoch 3/3\n",
      "126976/126976 [==============================] - 67s 527us/sample - loss: 0.6374 - val_loss: 0.6123\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 64s 526us/sample - loss: 0.6487 - val_loss: 0.6286\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 60s 493us/sample - loss: 0.6484 - val_loss: 0.6286\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 61s 497us/sample - loss: 0.6481 - val_loss: 0.6285\n",
      "Train on 117760 samples, validate on 29440 samples\n",
      "Epoch 1/3\n",
      "117760/117760 [==============================] - 59s 500us/sample - loss: 0.6676 - val_loss: 0.6686\n",
      "Epoch 2/3\n",
      "117760/117760 [==============================] - 58s 493us/sample - loss: 0.6671 - val_loss: 0.6684\n",
      "Epoch 3/3\n",
      "117760/117760 [==============================] - 58s 493us/sample - loss: 0.6668 - val_loss: 0.6684\n",
      "Train on 124928 samples, validate on 31232 samples\n",
      "Epoch 1/3\n",
      "124928/124928 [==============================] - 64s 510us/sample - loss: 0.6763 - val_loss: 0.6618\n",
      "Epoch 2/3\n",
      "124928/124928 [==============================] - 62s 494us/sample - loss: 0.6759 - val_loss: 0.6616\n",
      "Epoch 3/3\n",
      "124928/124928 [==============================] - 61s 491us/sample - loss: 0.6756 - val_loss: 0.6617\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 63s 489us/sample - loss: 0.6563 - val_loss: 0.6534\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 65s 509us/sample - loss: 0.6559 - val_loss: 0.6534\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 62s 485us/sample - loss: 0.6556 - val_loss: 0.6534\n",
      "Train on 123904 samples, validate on 30976 samples\n",
      "Epoch 1/3\n",
      "123904/123904 [==============================] - 64s 518us/sample - loss: 0.6699 - val_loss: 0.6648\n",
      "Epoch 2/3\n",
      "123904/123904 [==============================] - 64s 516us/sample - loss: 0.6695 - val_loss: 0.6648\n",
      "Epoch 3/3\n",
      "123904/123904 [==============================] - 61s 494us/sample - loss: 0.6692 - val_loss: 0.6649\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 63s 495us/sample - loss: 0.6496 - val_loss: 0.5686\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 66s 514us/sample - loss: 0.6493 - val_loss: 0.5686\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 63s 489us/sample - loss: 0.6491 - val_loss: 0.5687\n",
      "Train on 123904 samples, validate on 30976 samples\n",
      "Epoch 1/3\n",
      "123904/123904 [==============================] - 62s 504us/sample - loss: 0.6307 - val_loss: 0.6616\n",
      "Epoch 2/3\n",
      "123904/123904 [==============================] - 61s 496us/sample - loss: 0.6302 - val_loss: 0.6619\n",
      "Epoch 3/3\n",
      "123904/123904 [==============================] - 60s 488us/sample - loss: 0.6299 - val_loss: 0.6621\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2851d32426432b94c6004544ca6d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125952 samples, validate on 31488 samples\n",
      "Epoch 1/3\n",
      "125952/125952 [==============================] - 63s 497us/sample - loss: 0.6347 - val_loss: 0.6244\n",
      "Epoch 2/3\n",
      "125952/125952 [==============================] - 63s 500us/sample - loss: 0.6344 - val_loss: 0.6243\n",
      "Epoch 3/3\n",
      "125952/125952 [==============================] - 62s 494us/sample - loss: 0.6341 - val_loss: 0.6244\n",
      "Train on 130048 samples, validate on 32512 samples\n",
      "Epoch 1/3\n",
      "130048/130048 [==============================] - 65s 501us/sample - loss: 0.6350 - val_loss: 0.6826\n",
      "Epoch 2/3\n",
      "130048/130048 [==============================] - 63s 484us/sample - loss: 0.6347 - val_loss: 0.6827\n",
      "Epoch 3/3\n",
      "130048/130048 [==============================] - 64s 490us/sample - loss: 0.6345 - val_loss: 0.6827\n",
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/3\n",
      "128000/128000 [==============================] - 74s 579us/sample - loss: 0.6639 - val_loss: 0.6496\n",
      "Epoch 2/3\n",
      "128000/128000 [==============================] - 69s 537us/sample - loss: 0.6637 - val_loss: 0.6497\n",
      "Epoch 3/3\n",
      "128000/128000 [==============================] - 63s 491us/sample - loss: 0.6634 - val_loss: 0.6498\n",
      "Train on 125952 samples, validate on 31488 samples\n",
      "Epoch 1/3\n",
      "125952/125952 [==============================] - 63s 502us/sample - loss: 0.6468 - val_loss: 0.6301\n",
      "Epoch 2/3\n",
      "125952/125952 [==============================] - 64s 509us/sample - loss: 0.6466 - val_loss: 0.6300\n",
      "Epoch 3/3\n",
      "125952/125952 [==============================] - 62s 490us/sample - loss: 0.6464 - val_loss: 0.6300\n",
      "Train on 132096 samples, validate on 33024 samples\n",
      "Epoch 1/3\n",
      "132096/132096 [==============================] - 69s 522us/sample - loss: 0.6645 - val_loss: 0.6829\n",
      "Epoch 2/3\n",
      "132096/132096 [==============================] - 67s 509us/sample - loss: 0.6643 - val_loss: 0.6828\n",
      "Epoch 3/3\n",
      "132096/132096 [==============================] - 67s 505us/sample - loss: 0.6641 - val_loss: 0.6827\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 60s 493us/sample - loss: 0.6691 - val_loss: 0.6721\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 60s 495us/sample - loss: 0.6690 - val_loss: 0.6721\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 61s 500us/sample - loss: 0.6688 - val_loss: 0.6720\n",
      "Train on 121856 samples, validate on 30464 samples\n",
      "Epoch 1/3\n",
      "121856/121856 [==============================] - 67s 547us/sample - loss: 0.6311 - val_loss: 0.6849\n",
      "Epoch 2/3\n",
      "121856/121856 [==============================] - 61s 502us/sample - loss: 0.6307 - val_loss: 0.6850\n",
      "Epoch 3/3\n",
      "121856/121856 [==============================] - 60s 491us/sample - loss: 0.6305 - val_loss: 0.6850\n",
      "Train on 125235 samples, validate on 31309 samples\n",
      "Epoch 1/3\n",
      "125235/125235 [==============================] - 78s 624us/sample - loss: 0.6505 - val_loss: 0.6555\n",
      "Epoch 2/3\n",
      "125235/125235 [==============================] - 71s 568us/sample - loss: 0.6502 - val_loss: 0.6555\n",
      "Epoch 3/3\n",
      "125235/125235 [==============================] - 71s 564us/sample - loss: 0.6500 - val_loss: 0.6556\n",
      "Train on 130048 samples, validate on 32512 samples\n",
      "Epoch 1/3\n",
      " 55360/130048 [===========>..................] - ETA: 33s - loss: 0.6533"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9f2466c05161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;31m# use when randomly selecting batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m         \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "histories = list()\n",
    "for _ in range(4):\n",
    "    for batch_path in tqdm(glob(join(training_path, \"*\"))):\n",
    "        batch = pickle.load(open(batch_path, \"rb\"))\n",
    "        bold_signal = batch.pop(\"pred\")\n",
    "        model.fit(\n",
    "            batch, bold_signal,\n",
    "            epochs=num_epoches, batch_size=batch_size, verbose=True,\n",
    "            callbacks=[tensorboard_callback, history],\n",
    "            validation_split=0.2,\n",
    "            shuffle=True # use when randomly selecting batches\n",
    "        )\n",
    "        histories.append(model.history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = {\"loss\": [], \"val_loss\": []}\n",
    "for h in histories:\n",
    "    tmp[\"loss\"].extend(h[\"loss\"])\n",
    "    tmp[\"val_loss\"].extend(h[\"val_loss\"])\n",
    "    \n",
    "pd.DataFrame(tmp).to_csv(\"/Users/pstetz/Desktop/confidential/.project/4_65_score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/Users/pstetz/Desktop/confidential/.project/run/4_65_score.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
