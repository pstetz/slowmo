{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "from os.path import join, isfile\n",
    "from random import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "tf.random.set_seed(5)\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "root = \"/Users/pstetz/Desktop/confidential/.project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(history_path, data):\n",
    "    if isfile(history_path):\n",
    "        pd.concat([pd.read_csv(history_path), pd.DataFrame(data)]).to_csv(history_path, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(data).to_csv(history_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "<div hidden>\n",
    "def _load_model():\n",
    "    lr = LeakyReLU(alpha=0.05); lr.__name__ = 'leaky_relu'\n",
    "    relu = ReLU(); relu.__name__ = \"relu\"\n",
    "    activation = relu\n",
    "    use_bias = False\n",
    "    def layer_a(dim):\n",
    "        return layers.Dense(dim, activation=activation, use_bias=use_bias)\n",
    "    \n",
    "    info_input = keras.Input(shape=(130,), name=\"info\")\n",
    "    prev_input = keras.Input(shape=(9, 9, 9, 2), name=\"prev\")\n",
    "    next_input = keras.Input(shape=(9, 9, 9, 2), name=\"next\")\n",
    "    prev_s, next_s = [prev_input], [next_input]\n",
    "    for i, layer in enumerate([\n",
    "        layers.Conv3D(8, (2, 2, 2), use_bias=False),\n",
    "        layers.MaxPool3D(),\n",
    "        layers.Conv3D(8, (2, 2, 2), use_bias=False),\n",
    "        layers.Conv3D(8, (2, 2, 2), use_bias=False),\n",
    "        layers.Flatten()\n",
    "    ]):\n",
    "        prev_s.append(layer(prev_s[-1]))\n",
    "        next_s.append(layer(next_s[-1]))\n",
    "    info_s = [info_input]\n",
    "#     for dim in (130, 130, 130):\n",
    "#         info_s.append(layer_a(dim)(info_s[-1]))\n",
    "\n",
    "    x_0 = layers.concatenate([prev_s[-1], next_s[-1], info_s[-1]])\n",
    "    x_s = [x_0]\n",
    "#     for dim in (146, 146, 146, 146, 128, 64, 32):\n",
    "    for dim in (258, 258, 128, 128, 64, 32):\n",
    "        x_s.append(layer_a(dim)(x_s[-1]))\n",
    "    \n",
    "    bold_signal = layers.Dense(1, name=\"bold_signal\")(x_s[-1])\n",
    "    model = keras.Model(inputs=[prev_input, next_input, info_input], outputs=[bold_signal])\n",
    "    learning_rate = 1e-4\n",
    "    model.compile(optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=8e-2, decay=learning_rate/30),\n",
    "      loss={\"bold_signal\": \"mse\"},\n",
    "      loss_weights=[1.])\n",
    "    return model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model(with_lgbm = False):\n",
    "    lr = LeakyReLU(alpha=0.05); lr.__name__ = 'leaky_relu'\n",
    "    relu = ReLU(); relu.__name__ = \"relu\"\n",
    "#     activation = lr\n",
    "    \n",
    "    info_input = keras.Input(shape=(130 + int(with_lgbm),), name=\"info\")\n",
    "    prev_input = keras.Input(shape=(9, 9, 9, 2), name=\"prev\")\n",
    "    next_input = keras.Input(shape=(9, 9, 9, 2), name=\"next\")\n",
    "    prev_s, next_s = [prev_input], [next_input]\n",
    "    for i, layer in enumerate([\n",
    "        layers.Conv3D(8, (2, 2, 2), use_bias=False),\n",
    "        layers.Conv3D(8, (2, 2, 2), use_bias=False),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Flatten()\n",
    "    ]):\n",
    "        prev_s.append(layer(prev_s[-1]))\n",
    "        next_s.append(layer(next_s[-1]))\n",
    "    info_s = [info_input]\n",
    "    x_0 = layers.concatenate([prev_s[-1], next_s[-1], info_s[-1]])\n",
    "    x_s = [x_0]\n",
    "    init_shape = x_0.shape[1]\n",
    "    for layer in (\n",
    "        layers.Reshape((init_shape, 1)),\n",
    "        layers.Conv1D(256, init_shape, strides=init_shape, activation='elu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv1D(128, 1, activation='elu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv1D(64, 1, activation='elu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Reshape((64, 1)),\n",
    "        layers.AveragePooling1D(2),\n",
    "        layers.Flatten(),\n",
    "    ):\n",
    "#         print(x_s[-1].shape)\n",
    "        x_s.append(layer(x_s[-1]))\n",
    "    \n",
    "    bold_signal = layers.Dense(1, name=\"bold_signal\")(x_s[-1])\n",
    "    model = keras.Model(inputs=[prev_input, next_input, info_input], outputs=[bold_signal])\n",
    "    learning_rate = 1e-3\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=8e-1, decay=learning_rate/30),\n",
    "        loss={\"bold_signal\": \"mse\"}, loss_weights=[1.]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "if \"model\" in locals(): del model\n",
    "model = _load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = join(root, \"history.csv\")\n",
    "training_path = join(root, \"training\")\n",
    "log_dir = join(root, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "train_batch  = 1\n",
    "input_batch  = train_batch * 2\n",
    "num_epoches = 2\n",
    "\n",
    "if isfile(history_path):\n",
    "    os.remove(history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d26672ea514901a2db1549b9b745a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=24513.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 20s 300us/sample - loss: 0.8857\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 19s 293us/sample - loss: 0.7645\n",
      "0.765 0.7301344275474548\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 270us/sample - loss: 0.7439\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 273us/sample - loss: 0.7336\n",
      "0.734 0.7323139309883118\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 270us/sample - loss: 0.7432\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 278us/sample - loss: 0.7351\n",
      "0.735 0.7410476803779602\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 271us/sample - loss: 0.7145\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 272us/sample - loss: 0.7094\n",
      "0.709 0.7308624386787415\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 17s 261us/sample - loss: 0.7140\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 269us/sample - loss: 0.7102\n",
      "0.71 0.7232924699783325\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 267us/sample - loss: 0.7215\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 276us/sample - loss: 0.7190\n",
      "0.719 0.7414573431015015\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 273us/sample - loss: 0.7000\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 273us/sample - loss: 0.6974\n",
      "0.697 0.7265608310699463\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 277us/sample - loss: 0.7145\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 278us/sample - loss: 0.7116\n",
      "0.712 0.7448256015777588\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 274us/sample - loss: 0.7085\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 17s 262us/sample - loss: 0.7072\n",
      "0.707 0.7396379709243774\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 274us/sample - loss: 0.7043\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 274us/sample - loss: 0.7032\n",
      "0.703 0.7458042502403259\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 276us/sample - loss: 0.6991\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 19s 287us/sample - loss: 0.6980\n",
      "0.698 0.7364999055862427\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 277us/sample - loss: 0.7007\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 274us/sample - loss: 0.6987\n",
      "0.699 0.7358900904655457\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 274us/sample - loss: 0.6927\n",
      "Epoch 2/2\n",
      "65536/65536 [==============================] - 18s 282us/sample - loss: 0.6921\n",
      "0.692 0.7283025979995728\n",
      "Train on 65536 samples\n",
      "Epoch 1/2\n",
      "65536/65536 [==============================] - 18s 275us/sample - loss: 0.6944\n",
      "Epoch 2/2\n",
      " 4096/65536 [>.............................] - ETA: 17s - loss: 0.6600"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-88328341d3d6>\", line 25, in <module>\n",
      "    shuffle=True # use when randomly selecting batches\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\n",
      "    total_epochs=epochs)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\n",
      "    batch_outs = execution_function(iterator)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\n",
      "    distributed_function(input_fn))\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 487, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\n",
      "    ctx=ctx)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\n",
      "    num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/Users/pstetz/anaconda3/envs/tf/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "bold_signal, prev_volume, next_volume, info = [], [], [], []\n",
    "\n",
    "for _ in range(4):\n",
    "    training = glob(join(training_path, \"*\", \"*\"))\n",
    "    shuffle(training)\n",
    "    for i, batch_path in enumerate(tqdm(training)):\n",
    "        if not isfile(join(batch_path, \"norm_info.npy\")):\n",
    "            print(\"%s not found\" % batch_path)\n",
    "            continue\n",
    "        bold_signal.extend(np.load(join(batch_path, \"pred.npy\")))\n",
    "        prev_volume.extend(np.load(join(batch_path, \"norm_prev.npy\")))\n",
    "        next_volume.extend(np.load(join(batch_path, \"norm_next.npy\")))\n",
    "        info.extend(np.load(join(batch_path, \"norm_info.npy\"), allow_pickle=True))\n",
    "        if (i + 1) % input_batch == 0:\n",
    "            batch = {\n",
    "                \"prev\": np.array(prev_volume),\n",
    "                \"next\": np.array(next_volume),\n",
    "                \"info\": np.array(info, dtype=np.float32)\n",
    "            }\n",
    "            bold_signal = np.array(bold_signal)\n",
    "            history = model.fit(\n",
    "                batch, bold_signal,\n",
    "                epochs=num_epoches, batch_size=train_batch, verbose=True,\n",
    "                callbacks=[tensorboard_callback],\n",
    "                shuffle=True # use when randomly selecting batches\n",
    "            )\n",
    "            _std = np.std(bold_signal).round(3)\n",
    "            _loss = history.history[\"loss\"][-1].round(3)\n",
    "            assert not np.isnan(_loss), batch_path\n",
    "            _mean = np.add(batch[\"prev\"][:, 4, 4, 4, 0], batch[\"next\"][:, 4, 4, 4, 0]) / 2\n",
    "            mean_loss = np.sum(np.square(np.subtract(_mean, bold_signal))) / len(bold_signal)\n",
    "            print(_loss, mean_loss)\n",
    "            data = {\n",
    "                \"std\": [_std] * num_epoches,\n",
    "                \"loss\": history.history[\"loss\"],\n",
    "                \"mean_loss\": mean_loss\n",
    "            }\n",
    "            save_history(history_path, data)\n",
    "            bold_signal, prev_volume, next_volume, info = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/Volumes/hd_4tb/results/3_more_filters.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
