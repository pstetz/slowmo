{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flywheel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom helpers\n",
    "import sys\n",
    "sys.path.append(\"classes\")\n",
    "\n",
    "# from Model import Model\n",
    "from Cloud import Cloud\n",
    "from Button import Button\n",
    "from DicomDir import DicomDir \n",
    "from helpers import *\n",
    "from Subject import Subject \n",
    "\n",
    "# ### Load objects\n",
    "cloud = Cloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0a9427395a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def build_model(num_features):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=tf.nn.relu, input_shape=(num_features,)),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(32, activation=tf.nn.relu),\n",
    "        layers.Dense(32, activation=tf.nn.relu),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mean_absolute_error', 'mean_squared_error'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on subject PA30563\n",
      "Training on task gonogo\n",
      "Downloading /Users/pbezuhov/tmp/engage-PA30563-000_data_archive-gonogo.dicom.zip...\n",
      "unzipping /Users/pbezuhov/tmp/engage-PA30563-000_data_archive-gonogo.dicom.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/6930 [00:00<01:46, 64.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing old path /Users/pbezuhov/tmp/engage-PA30563-000_data_archive-gonogo.dicom.zip...\n",
      "loading dicoms from /Users/pbezuhov/tmp/engage-PA30563-000_data_archive-gonogo...\n",
      "Sorting dicoms by trigger time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6930/6930 [02:01<00:00, 57.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on volume 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14336/14336 [04:08<00:00, 57.73it/s]\n",
      "100%|██████████| 1050/1050 [00:20<00:00, 51.26it/s] \n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0902 20:12:49.995352 4619183552 training.py:510] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on volume 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14336/14336 [04:12<00:00, 56.89it/s]\n",
      "100%|██████████| 1050/1050 [00:20<00:00, 50.26it/s] \n",
      "W0902 20:17:54.129072 4619183552 training.py:510] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on volume 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14336/14336 [04:12<00:00, 56.83it/s]\n",
      "100%|██████████| 1050/1050 [00:21<00:00, 49.51it/s] \n",
      "W0902 20:22:58.253246 4619183552 training.py:510] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on volume 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14336/14336 [04:52<00:00, 48.99it/s]\n",
      "100%|██████████| 1050/1050 [00:24<00:00, 42.44it/s] \n",
      "W0902 20:28:48.085379 4619183552 training.py:510] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on volume 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14336/14336 [04:48<00:00, 49.66it/s]\n",
      "100%|██████████| 1050/1050 [00:20<00:00, 51.94it/s] \n",
      "W0902 20:34:32.747190 4619183552 training.py:510] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on volume 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 175/14336 [00:03<04:20, 54.26it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "histories = list()\n",
    "\n",
    "model = None\n",
    "projects = [\"engage\"]\n",
    "tasks = [\"gonogo\", \"conscious\", \"nonconscious\"]\n",
    "time_sessions = [\"000_data_archive\"]\n",
    "\n",
    "### Iterate over projects\n",
    "for project in projects:\n",
    "#         time_sessions = project_config[\"time_sessions\"]\n",
    "#         subjects = load_project_subjects(project)\n",
    "    subjects = pd.read_csv(\"/Volumes/group/PANLab_Datasets/ENGAGE/ENGAGE_BV_sublist_n108_Final_List.csv\")\n",
    "    subjects = subjects.subNum\n",
    "\n",
    "    ### Iterate over time_session/subject\n",
    "    for time_session in time_sessions:\n",
    "        for subject_id in subjects:\n",
    "            print(\"Training on subject %s\" % subject_id)\n",
    "            ### Set logger info\n",
    "#                 logger.set(project=project, subject=subject, time_session=time_session, task=task)\n",
    "\n",
    "            ### Load subject specific data\n",
    "            subject = Subject(subject_id)\n",
    "\n",
    "            ### Load the button load files\n",
    "            buttons = Button()\n",
    "            buttons.find_subject_session_logs(project, subject_id, time_session)\n",
    "            \n",
    "            ### Skip subject if they have no onsets\n",
    "            if len(buttons.onsets) == 0:\n",
    "                print(\"No button logs for %s %s\" % (subject_id, time_session))\n",
    "                continue\n",
    "                \n",
    "            for i, task in enumerate(tasks):\n",
    "                print(\"Training on task %s\" % task)\n",
    "                \n",
    "                ###\n",
    "                onsets = buttons.onsets[i]\n",
    "                \n",
    "                ### Download task dicom from Flywheel\n",
    "                dicom_path = cloud.download(project, subject_id, time_session, task)\n",
    "\n",
    "                ### Load dicoms and cut the first 3 volumes\n",
    "                fmri_data = DicomDir(dicom_path)\n",
    "                fmri_data.cut_volumes(3)\n",
    "\n",
    "                for num_volume in range(1, fmri_data.num_volumes - 2):\n",
    "                    print(\"Training on volume %d\" % num_volume)\n",
    "                    \n",
    "                    ### Generate the training data from all the features\n",
    "                    df = gen_data(project, task, onsets, fmri_data, subject, num_volume)\n",
    "                    \n",
    "                    ### Convert float64 values to float32 to save memory\n",
    "                    df = convert_float64_to_float32(df)\n",
    "                    \n",
    "                    ### Get target values\n",
    "                    labels = df.pop(\"signal\")\n",
    "                    \n",
    "                    ### Initialize model\n",
    "                    if model is None:\n",
    "                        model = build_model(df.shape[1])\n",
    "                        \n",
    "                    ### Fit the model\n",
    "                    record = model.fit(x=df, y=labels, validation_split=0.2, verbose=False, epochs=EPOCHS)\n",
    "                    \n",
    "                    ### Save the model weights as a checkpoint\n",
    "                    model.save_weights('my_model_weights.h5')\n",
    "                    \n",
    "                    ### Save the history\n",
    "                    histories.append(pd.DataFrame(record.history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to save the history before overwriting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.concat(histories)\n",
    "history.to_csv(\"history1.csv\") ### FIXME: put datetime in name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
