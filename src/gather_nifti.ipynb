{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "### Custom helpers\n",
    "import sys\n",
    "sys.path.append(\"classes\")\n",
    "from Cloud import Cloud\n",
    "from Button import Button\n",
    "from DicomDir import DicomDir \n",
    "from helpers import *\n",
    "from Subject import Subject \n",
    "\n",
    "# ### Load objects\n",
    "cloud = Cloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save session JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(json_path, project, subject, task, is_ascending, operator):\n",
    "    import json\n",
    "    info = dict()\n",
    "    \n",
    "    ### JSON information\n",
    "    info[\"age\"]          = subject.age\n",
    "    info[\"sex\"]          = subject.sex\n",
    "    info[\"project\"]      = project\n",
    "    info[\"is_gng\"]       = task\n",
    "    info[\"is_ascending\"] = is_ascending\n",
    "    info[\"operator\"]     = str(operator)\n",
    "    \n",
    "    ### Create directory (if needed)\n",
    "    dst_dir = os.path.dirname(json_path)\n",
    "    if not os.path.isdir(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    \n",
    "    ### Save JSON\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on subject PA30563\n",
      "Training on task gonogo\n",
      "Downloading /Users/pbezuhov/tmp/engage-PA30563-000_data_archive-gonogo.dicom.zip...\n",
      "unzipping /Users/pbezuhov/tmp/engage-PA30563-000_data_archive-gonogo.dicom.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/6930 [00:00<02:10, 53.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing old path /Users/pbezuhov/tmp/engage-PA30563-000_data_archive-gonogo.dicom.zip...\n",
      "loading dicoms from /Users/pbezuhov/tmp/engage-PA30563-000_data_archive-gonogo...\n",
      "Sorting dicoms by trigger time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6930/6930 [02:01<00:00, 56.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for volume 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143360/143360 [47:35<00:00, 50.21it/s]  \n",
      "100%|██████████| 1041/1041 [08:47<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for volume 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 80042/143360 [26:46<23:56, 44.07it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-801317a2dc70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;31m### Generate the training data from all the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmri_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_volume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mfilename\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"volume_%d.feather\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_volume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/MRI-SlowMo/src/helpers.py\u001b[0m in \u001b[0;36mgen_data\u001b[0;34m(dicoms, num_volume, onsets, radius, percent_sample, low_memory)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mtrigger_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdicoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trigger_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_volume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mstimuli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecent_onsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monset_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrigger_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mstimuli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_onsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimuli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0m_add_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/MRI-SlowMo/src/helpers.py\u001b[0m in \u001b[0;36mrecent_onsets\u001b[0;34m(df, trigger_time)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0monset_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrigger_time\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mrecent_onset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_recent_onset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monset_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0monset_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrigger_time\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecent_onset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0monset_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/MRI-SlowMo/src/helpers.py\u001b[0m in \u001b[0;36mmost_recent_onset\u001b[0;34m(df, onset_names, time)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mmost_recent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0monset_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mrecent_ons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ons\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_ons\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 raise TypeError(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_datetimelike_v_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "projects = [\"engage\"]\n",
    "tasks = [\"gonogo\", \"conscious\", \"nonconscious\"]\n",
    "time_sessions = [\"000_data_archive\"]\n",
    "output = \"/Volumes/hd_4tb\"\n",
    "\n",
    "### Iterate over projects\n",
    "for project in projects:\n",
    "#         time_sessions = project_config[\"time_sessions\"]\n",
    "#         subjects = load_project_subjects(project)\n",
    "    subjects = pd.read_csv(\"/Volumes/group/PANLab_Datasets/ENGAGE/ENGAGE_BV_sublist_n108_Final_List.csv\")\n",
    "    subjects = subjects.subNum\n",
    "\n",
    "    ### Iterate over time_session/subject\n",
    "    for time_session in time_sessions:\n",
    "        for subject_id in subjects:\n",
    "            print(\"Generating data for subject %s\" % subject_id)\n",
    "\n",
    "            ### Load subject specific data\n",
    "            subject = Subject(subject_id)\n",
    "\n",
    "            ### Load the button load files\n",
    "            buttons = Button()\n",
    "            buttons.find_subject_session_logs(project, subject_id, time_session)\n",
    "            \n",
    "            ### Skip subject if they have no onsets\n",
    "            if len(buttons.onsets) == 0:\n",
    "                print(\"No button logs for %s %s\" % (subject_id, time_session))\n",
    "                continue\n",
    "                \n",
    "            for i, task in enumerate(tasks):\n",
    "                print(\"Generating data for task %s\" % task)\n",
    "                \n",
    "                ### Get the correct onsets from the ones gathered\n",
    "                onsets = buttons.onsets[i]\n",
    "                \n",
    "                ### Download task dicom from Flywheel\n",
    "                dicom_path = cloud.download(project, subject_id, time_session, task)\n",
    "\n",
    "                ### Load dicoms and cut the first 3 volumes\n",
    "                fmri_data = DicomDir(dicom_path)\n",
    "                fmri_data.cut_volumes(3)\n",
    "                \n",
    "                ### Generate JSON for patient - time_session - task\n",
    "                json_path = join(output, \"nifti\", project, time_session, subject_id, task, \"info.json\")\n",
    "                if not os.path.isfile(json_path):\n",
    "                    gen_json(json_path, project, subject, task, fmri_data.is_ascending, fmri_data.operator)\n",
    "\n",
    "                for num_volume in range(1, fmri_data.num_volumes - 2):\n",
    "                    print(\"Generating data for volume %d\" % num_volume)\n",
    "                    \n",
    "                    ### Generate the training data from all the features\n",
    "                    df = gen_data(fmri_data, num_volume, onsets)\n",
    "                    \n",
    "                    filename  = \"volume_%d.feather\" % num_volume\n",
    "                    save_path = join(output, \"nifti\", project, time_session, subject_id, task, filename)\n",
    "                    df.to_feather(save_path)\n",
    "                    del df\n",
    "                    \n",
    "                ### Delete dicom data\n",
    "                shutil.rmtree(fmri_data.directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
