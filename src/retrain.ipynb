{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model():\n",
    "    info_input = keras.Input(shape=(130,), name=\"info\")\n",
    "    prev_input = keras.Input(shape=(9, 9, 9, 2), name=\"prev\")\n",
    "    next_input = keras.Input(shape=(9, 9, 9, 2), name=\"next\")\n",
    "\n",
    "    prev_s, next_s = [prev_input], [next_input]\n",
    "    for layer in [\n",
    "#         layers.Conv3D(2, (3, 3, 3)),\n",
    "#         layers.Conv3D(2, (2, 2, 2)),\n",
    "        layers.Flatten()\n",
    "    ]:\n",
    "        prev_s.append(layer(prev_s[-1]))\n",
    "        next_s.append(layer(next_s[-1]))\n",
    "    \n",
    "    info_s = [info_input]\n",
    "    for layer in [\n",
    "#         layers.Dense(130),\n",
    "#         layers.Dense(130),\n",
    "#         layers.Dense(130),\n",
    "#         layers.Dense(130)\n",
    "    ]:\n",
    "        info_s.append(layer(info_s[-1]))\n",
    "\n",
    "    x_0 = layers.concatenate([prev_s[-1], next_s[-1], info_s[-1]])\n",
    "    x_s = [x_0]\n",
    "    for layer in [\n",
    "        layers.Dense(994),\n",
    "        layers.Dense(994),\n",
    "#         layers.Dropout(0.25),\n",
    "        layers.Dense(800),\n",
    "        layers.Dense(800),\n",
    "#         layers.Dropout(0.25),\n",
    "        layers.Dense(512),\n",
    "        layers.Dense(256),\n",
    "        layers.Dense(256),\n",
    "        layers.Dense(128),\n",
    "        layers.Dense( 64),\n",
    "        layers.Dense( 32),\n",
    "    ]:\n",
    "        x_s.append(layer(x_s[-1]))\n",
    "    \n",
    "    bold_signal = layers.Dense(1, activation=\"relu\", name=\"bold_signal\")(x_s[-1])\n",
    "\n",
    "    model = keras.Model(inputs=[prev_input, next_input, info_input], outputs=[bold_signal])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "          loss={\"bold_signal\": \"mean_squared_error\"},\n",
    "          loss_weights=[1.])\n",
    "    return model\n",
    "\n",
    "model = _load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = \"/Volumes/hd_4tb/results/history.csv\"\n",
    "checkpoint_path = \"/Volumes/hd_4tb/results/checkpoints\"\n",
    "training_path = \"/Volumes/hd_4tb/results/training\"\n",
    "batch_size  = 256\n",
    "num_epoches = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cffbaccd9f14eee94be18e6e70e01a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=654), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "bold_signal, prev_volume, next_volume, info = [], [], [], []\n",
    "for i, batch in enumerate(tqdm(glob(join(training_path, \"*\")))):\n",
    "    bold_signal.extend(np.load(join(batch, \"pred.npy\")))\n",
    "    prev_volume.extend(np.load(join(batch, \"prev.npy\")))\n",
    "    next_volume.extend(np.load(join(batch, \"next.npy\")))\n",
    "    info.extend(np.load(join(batch, \"info.npy\"), allow_pickle=True))\n",
    "    if (i + 1) % 6 == 0:        \n",
    "        batch = {\"prev\": np.array(prev_volume), \"next\": np.array(next_volume), \"info\": np.array(info)}\n",
    "        history = model.fit(\n",
    "            batch, {'bold_signal': np.array(bold_signal)},\n",
    "            epochs=num_epoches, batch_size=batch_size, verbose=False,\n",
    "#             shuffle=True # use when randomly selecting batches\n",
    "        )\n",
    "        _std = np.std(bold_signal).round(3)\n",
    "        _loss = history.history[\"loss\"][0].round(3)\n",
    "        if _loss > 10:\n",
    "            print(i)\n",
    "        data = {\"std\": [_std], \"loss\": [_loss]}\n",
    "        if os.path.isfile(history_path):\n",
    "            pd.concat([pd.read_csv(history_path), pd.DataFrame(data)]).to_csv(history_path, index=False)\n",
    "        else:\n",
    "            pd.DataFrame(data).to_csv(history_path, index=False)\n",
    "        bold_signal, prev_volume, next_volume, info = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
