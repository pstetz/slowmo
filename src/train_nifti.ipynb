{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grey masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in_001716': 1, 'in_013136': 0, 'in_073720': 0, 'in_125909': 0, 'in_131520': 0, 'in_156999': 0, 'in_176064': 0, 'in_182355': 0, 'in_216192': 0, 'in_275836': 0, 'in_291082': 0, 'in_414207': 0, 'in_426426': 0, 'in_447647': 0, 'in_450767': 0, 'in_477522': 0, 'in_525931': 0, 'in_532911': 0, 'in_533294': 0, 'in_534482': 0, 'in_707656': 0, 'in_752691': 0, 'in_779062': 0, 'in_791103': 0, 'in_831650': 0, 'in_877949': 0, 'in_911981': 0, 'in_954861': 0}\n"
     ]
    }
   ],
   "source": [
    "masks = glob(\"/Volumes/hd_4tb/masks/plip/*\")\n",
    "masks = [{\n",
    "    \"code\": os.path.basename(mask).split(\"_\")[0],\n",
    "    \"data\": nib.load(mask).get_data(),\n",
    "} for mask in masks]\n",
    "\n",
    "def _in_mask(masks, x, y, z):\n",
    "    result = dict()\n",
    "    for mask in masks:\n",
    "        code = mask[\"code\"]\n",
    "        data = mask[\"data\"]\n",
    "        result[\"in_%s\" % code] = int(bool(data[x, y, z]))\n",
    "    return result\n",
    "\n",
    "def _mean_activation(masks, fmri, grey, t, label):\n",
    "    activations = dict()\n",
    "    for mask in masks:\n",
    "        code, data = mask[\"code\"], mask[\"data\"]\n",
    "        region = np.multiply(data, fmri[:, :, :, t])\n",
    "        activations[\"mean_%s_%s\" % (code, label)] = np.mean( np.multiply(grey, region) )\n",
    "    return activations\n",
    "\n",
    "def _mask_info(masks, fmri, grey, coord):\n",
    "    x, y, z, t = coord\n",
    "    info = _in_mask(masks, x, y, z)\n",
    "    for timepoint, label in [(t-1, \"prev\"), (t+1, \"next\")]:\n",
    "        info = {**info, **_mean_activation(masks, fmri, grey, timepoint, label)}\n",
    "    return info\n",
    "\n",
    "print(_in_mask(masks, 49, 66, 33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_volume(fmri, x, y, z, t):\n",
    "    volume = nii_input(fmri[:, :, :, t], x, y, z)\n",
    "    return np.array(volume)\n",
    "\n",
    "def _load(filepath):\n",
    "    return nib.as_closest_canonical(nib.load(filepath))\n",
    "\n",
    "def _get_data(filepath):\n",
    "    image = _load(filepath)\n",
    "    return image.get_data()\n",
    "\n",
    "def _time_map(session):\n",
    "    _map = {\n",
    "        \"s1\": \"000\", \"s2\": \"2MO\", \"s3\": \"6MO\", \"s4\": \"12MO\", \"s5\": \"24MO\"\n",
    "    }\n",
    "    return _map[session]\n",
    "\n",
    "def _get(row, item):\n",
    "    return row[item]\n",
    "\n",
    "def _fmri_path(row):\n",
    "    _input = \"/Volumes/hd_4tb/raw\"\n",
    "    project = _get(row, \"project\")\n",
    "    subject = _get(row, \"subject\")\n",
    "    time_session = _get(row, \"time_session\")\n",
    "    task = _get(row, \"task\")\n",
    "    return join(_input, project, _time_map(time_session), subject, task)\n",
    "\n",
    "def nii_input(data, x, y, z, r = 4):\n",
    "    return data[\n",
    "        x - r : x + r + 1,\n",
    "        y - r : y + r + 1,\n",
    "        z - r : z + r + 1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _onset_time(onsets, curr_time):\n",
    "    onset = onsets[onsets[\"ons\"] < curr_time]\n",
    "    onset = onset.sort_values(\"ons\", ascending=False)\n",
    "    if len(onset) > 0:\n",
    "        _time = onset.iloc[0].ons\n",
    "        return curr_time - _time\n",
    "    return False\n",
    "\n",
    "def _stim_time(df, stimuli, curr_time):\n",
    "    stim = df[df.category == stimuli]\n",
    "    return _onset_time(stim, curr_time)\n",
    "\n",
    "def _keypress_times(df, button, curr_time):\n",
    "    keys = df[(\n",
    "        (df[\"category\"] == \"keypress\") &\n",
    "        (df[\"stimulus\"] == button)\n",
    "    )]\n",
    "    return _onset_time(keys, curr_time)\n",
    "\n",
    "def last_onset(onset_df, task, curr_time, max_time=1000):\n",
    "    task_stimuli = {\n",
    "        \"gonogo\":       [\"Go\", \"NoGo\"],\n",
    "        \"conscious\":    [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\"],\n",
    "        \"nonconscious\": [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\"],\n",
    "        \"workingmemSB\": [\"Baseline\", \"NonTarget\", \"Target\"],\n",
    "        \"workingmemMB\": [\"Baseline\", \"NonTarget\", \"Target\"],\n",
    "    }\n",
    "    all_stimuli  = set(np.concatenate(list(task_stimuli.values())))\n",
    "    onset_timing = {stimuli: max_time for stimuli in all_stimuli}\n",
    "    for button in [\"1\", \"6\"]:\n",
    "        onset_timing[button] = _keypress_times(onset_df, button, curr_time)\n",
    "        \n",
    "    for stimuli in task_stimuli[task]:\n",
    "        _time = _stim_time(onset_df, stimuli, curr_time)\n",
    "        if _time:\n",
    "            onset_timing[stimuli] = _time\n",
    "    return pd.DataFrame(onset_timing, index=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Add mean mask activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _checkpoint():\n",
    "    return False\n",
    "\n",
    "def _load_model():\n",
    "    info_input = keras.Input(shape=(126,), name=\"info\")\n",
    "    prev_input = keras.Input(shape=(9, 9, 9, 2), name=\"prev\")\n",
    "    next_input = keras.Input(shape=(9, 9, 9, 2), name=\"next\")\n",
    "\n",
    "    prev_conv1 = layers.Conv3D(1, (2, 2, 2))(prev_input)\n",
    "    next_conv1 = layers.Conv3D(1, (2, 2, 2))(next_input)\n",
    "    prev_conv2 = layers.Conv3D(4, (2, 2, 2))(prev_conv1)\n",
    "    next_conv2 = layers.Conv3D(4, (2, 2, 2))(next_conv1)\n",
    "    \n",
    "    info      = layers.Dense(126)(info_input)\n",
    "    flat_prev = layers.Flatten()(prev_conv2)\n",
    "    flat_next = layers.Flatten()(next_conv2)\n",
    "\n",
    "    x_0 = layers.concatenate([flat_prev, flat_next, info])\n",
    "    x_1 = layers.Dense(812)(x_0)\n",
    "    x_2 = layers.Dense(406)(x_1)\n",
    "    x_3 = layers.Dense(406)(x_2)\n",
    "    x_4 = layers.Dense(203)(x_3)\n",
    "    x_5 = layers.Dense(203)(x_4)\n",
    "    x_6 = layers.Dense(100)(x_5)\n",
    "    x_7 = layers.Dense( 50)(x_6)\n",
    "    bold_signal = layers.Dense(1, activation=\"sigmoid\", name=\"bold_signal\")(x_7)\n",
    "\n",
    "    model = keras.Model(inputs=[prev_input, next_input, info_input], outputs=[bold_signal])\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(1e-5),\n",
    "          loss={\"bold_signal\": \"mean_squared_error\"},\n",
    "          loss_weights=[1.])\n",
    "#     if _checkpoint():\n",
    "#         model.load_weights(\"checkpoint_path\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df                = pd.read_csv(\"/Volumes/hd_4tb/project/model_input.csv\")\n",
    "available_volumes = np.load(\"available_volumes.npy\")\n",
    "train_cols        = [c for c in df.columns if c.startswith(\"is_\")] + [\"age\"]\n",
    "model = _load_model()\n",
    "batch_size  = 8\n",
    "num_epoches = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "For working with multiple inputs/outputs see [here](https://www.tensorflow.org/guide/keras/functional#models_with_multiple_inputs_and_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pbezuhov/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n",
      "/Users/pbezuhov/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb86df78dd9439ca4b9c3c8f50511a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=228316), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 4s 443ms/sample - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 12ms/sample - loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 14ms/sample - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 10ms/sample - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 12ms/sample - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 15ms/sample - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 12ms/sample - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 0.0000e+00\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 10ms/sample - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 15ms/sample - loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 12ms/sample - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 14ms/sample - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 18ms/sample - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 16ms/sample - loss: 0.0000e+00\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 10ms/sample - loss: 1.2899\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 1.2899\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 14ms/sample - loss: 1.2899\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 12ms/sample - loss: 1.2899\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 1.2899\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 13ms/sample - loss: 1.2899\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 16ms/sample - loss: 1.2899\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 15ms/sample - loss: 1.2899\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 15ms/sample - loss: 1.2899\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 16ms/sample - loss: 1.2899\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.4645\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 14ms/sample - loss: 0.4645\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 19ms/sample - loss: 0.4645\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 15ms/sample - loss: 0.4645\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 15ms/sample - loss: 0.4645\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 15ms/sample - loss: 0.4645\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 11ms/sample - loss: 0.4645\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ef1b2908c091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 model.fit(\n\u001b[1;32m     22\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'bold_signal'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbold_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 )\n\u001b[1;32m     25\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prev\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"next\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    TR = 2 if _get(row, \"is_mb\") == 0 else 0.71\n",
    "    fmri_path = _fmri_path(row)\n",
    "    fmri      = _get_data(join(fmri_path, \"normalized.nii.gz\"))\n",
    "    grey      = _get_data(join(fmri_path, \"..\", \"structural\", \"gm_probseg.nii.gz\"))\n",
    "    onset_df  = pd.read_csv(join(fmri_path, \"onsets.csv\"))\n",
    "    info      = row[train_cols]\n",
    "    \n",
    "    batch = {\"prev\": list(), \"next\": list()}\n",
    "    bold_signal, mask_rows = list(), list()\n",
    "    for t in range(1, fmri.shape[3]):\n",
    "        onsets = last_onset(onset_df, _get(row, \"task\"), TR * t, max_time=1000)\n",
    "        for x, y, z in tqdm(available_volumes):\n",
    "            if len(batch[\"prev\"]) == batch_size:\n",
    "                batch[\"info\"] = pd.concat([\n",
    "                    pd.concat([onsets] * batch_size).reset_index(drop=True),\n",
    "                    pd.concat([pd.DataFrame(info).T] * batch_size).reset_index(drop=True),\n",
    "                    pd.DataFrame(mask_rows)\n",
    "                ], axis=1)\n",
    "                batch[\"prev\"], batch[\"next\"] = np.array(batch[\"prev\"]), np.array(batch[\"next\"])\n",
    "                model.fit(\n",
    "                    batch, {'bold_signal': np.array(bold_signal)},\n",
    "                    epochs=num_epoches, batch_size=8\n",
    "                )\n",
    "                batch = {\"prev\": list(), \"next\": list()}\n",
    "                bold_signal, mask_rows = list(), list()\n",
    "            mask_rows.append( _mask_info(masks, fmri, grey, (x, y, z, t))  )\n",
    "            grey_data = nii_input(grey, x, y, z)\n",
    "            for name, timepoint in [(\"prev\", t-1), (\"next\", t+1)]:\n",
    "                batch[name].append(\n",
    "                    np.stack((_load_volume(fmri, x, y, z, timepoint), grey_data), axis=3)\n",
    "                )\n",
    "\n",
    "            bold_signal.append(fmri[x, y, z, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
