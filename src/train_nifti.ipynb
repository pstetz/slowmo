{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grey masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in_001716': 1, 'in_013136': 0, 'in_073720': 0, 'in_125909': 0, 'in_131520': 0, 'in_156999': 0, 'in_176064': 0, 'in_182355': 0, 'in_216192': 0, 'in_275836': 0, 'in_291082': 0, 'in_414207': 0, 'in_426426': 0, 'in_447647': 0, 'in_450767': 0, 'in_477522': 0, 'in_525931': 0, 'in_532911': 0, 'in_533294': 0, 'in_534482': 0, 'in_707656': 0, 'in_752691': 0, 'in_779062': 0, 'in_791103': 0, 'in_831650': 0, 'in_877949': 0, 'in_911981': 0, 'in_954861': 0}\n"
     ]
    }
   ],
   "source": [
    "masks = glob(\"/Volumes/hd_4tb/masks/plip/*\")\n",
    "masks = [{\n",
    "    \"code\": os.path.basename(mask).split(\"_\")[0],\n",
    "    \"data\": nib.load(mask).get_data(),\n",
    "} for mask in masks]\n",
    "\n",
    "def _in_mask(masks, x, y, z):\n",
    "    result = dict()\n",
    "    for mask in masks:\n",
    "        code = mask[\"code\"]\n",
    "        data = mask[\"data\"]\n",
    "        result[\"in_%s\" % code] = int(bool(data[x, y, z]))\n",
    "    return result\n",
    "\n",
    "def _mean_activation(masks, fmri, grey, t, label):\n",
    "    activations = dict()\n",
    "    for mask in masks:\n",
    "        code, data = mask[\"code\"], mask[\"data\"]\n",
    "        region = np.multiply(data, fmri[:, :, :, t])\n",
    "        activations[\"mean_%s_%s\" % (code, label)] = np.mean( np.multiply(grey, region) )\n",
    "    return activations\n",
    "\n",
    "def _mask_info(masks, fmri, grey, coord):\n",
    "    x, y, z, t = coord\n",
    "    info = _in_mask(masks, x, y, z)\n",
    "    for timepoint, label in [(t-1, \"prev\"), (t+1, \"next\")]:\n",
    "        info = {**info, **_mean_activation(masks, fmri, grey, timepoint, label)}\n",
    "    info[\"x\"] = x\n",
    "    info[\"y\"] = y\n",
    "    info[\"z\"] = z\n",
    "    info[\"t\"] = t\n",
    "    return info\n",
    "\n",
    "print(_in_mask(masks, 49, 66, 33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_volume(fmri, x, y, z, t):\n",
    "    volume = nii_input(fmri[:, :, :, t], x, y, z)\n",
    "    return np.array(volume)\n",
    "\n",
    "def _load(filepath):\n",
    "    return nib.as_closest_canonical(nib.load(filepath))\n",
    "\n",
    "def _get_data(filepath):\n",
    "    image = _load(filepath)\n",
    "    return image.get_data()\n",
    "\n",
    "def _time_map(session):\n",
    "    _map = {\n",
    "        \"s1\": \"000\", \"s2\": \"2MO\", \"s3\": \"6MO\", \"s4\": \"12MO\", \"s5\": \"24MO\"\n",
    "    }\n",
    "    return _map[session]\n",
    "\n",
    "def _get(row, item):\n",
    "    return row[item]\n",
    "\n",
    "def _fmri_path(row):\n",
    "    _input = \"/Volumes/hd_4tb/raw\"\n",
    "    project = _get(row, \"project\")\n",
    "    subject = _get(row, \"subject\")\n",
    "    time_session = _get(row, \"time_session\")\n",
    "    task = _get(row, \"task\")\n",
    "    return join(_input, project, _time_map(time_session), subject, task)\n",
    "\n",
    "def nii_input(data, x, y, z, r = 4):\n",
    "    return data[\n",
    "        x - r : x + r + 1,\n",
    "        y - r : y + r + 1,\n",
    "        z - r : z + r + 1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _onset_time(onsets, curr_time):\n",
    "    onset = onsets[onsets[\"ons\"] < curr_time]\n",
    "    onset = onset.sort_values(\"ons\", ascending=False)\n",
    "    if len(onset) > 0:\n",
    "        _time = onset.iloc[0].ons\n",
    "        return curr_time - _time\n",
    "    return False\n",
    "\n",
    "def _stim_time(df, stimuli, curr_time):\n",
    "    stim = df[df.category == stimuli]\n",
    "    return _onset_time(stim, curr_time)\n",
    "\n",
    "def _keypress_times(df, button, curr_time):\n",
    "    keys = df[(\n",
    "        (df[\"category\"] == \"keypress\") &\n",
    "        (df[\"stimulus\"] == button)\n",
    "    )]\n",
    "    return _onset_time(keys, curr_time)\n",
    "\n",
    "def last_onset(onset_df, task, curr_time, max_time=1000):\n",
    "    task_stimuli = {\n",
    "        \"gonogo\":       [\"Go\", \"NoGo\"],\n",
    "        \"conscious\":    [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\"],\n",
    "        \"nonconscious\": [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\"],\n",
    "        \"workingmemSB\": [\"Baseline\", \"NonTarget\", \"Target\"],\n",
    "        \"workingmemMB\": [\"Baseline\", \"NonTarget\", \"Target\"],\n",
    "    }\n",
    "    all_stimuli  = set(np.concatenate(list(task_stimuli.values())))\n",
    "    onset_timing = {stimuli: max_time for stimuli in all_stimuli}\n",
    "    for button in [\"1\", \"6\"]:\n",
    "        onset_timing[button] = _keypress_times(onset_df, button, curr_time)\n",
    "        \n",
    "    for stimuli in task_stimuli[task]:\n",
    "        _time = _stim_time(onset_df, stimuli, curr_time)\n",
    "        if _time:\n",
    "            onset_timing[stimuli] = _time\n",
    "    return pd.DataFrame(onset_timing, index=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Add mean mask activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _checkpoint():\n",
    "    return False\n",
    "\n",
    "def _load_model():\n",
    "    info_input = keras.Input(shape=(130,), name=\"info\")\n",
    "    prev_input = keras.Input(shape=(9, 9, 9, 2), name=\"prev\")\n",
    "    next_input = keras.Input(shape=(9, 9, 9, 2), name=\"next\")\n",
    "\n",
    "    prev_conv1 = layers.Conv3D(1, (2, 2, 2))(prev_input)\n",
    "    next_conv1 = layers.Conv3D(1, (2, 2, 2))(next_input)\n",
    "    prev_conv2 = layers.Conv3D(4, (2, 2, 2))(prev_conv1)\n",
    "    next_conv2 = layers.Conv3D(4, (2, 2, 2))(next_conv1)\n",
    "    \n",
    "    info      = layers.Dense(126)(info_input)\n",
    "    flat_prev = layers.Flatten()(prev_conv2)\n",
    "    flat_next = layers.Flatten()(next_conv2)\n",
    "\n",
    "    x_0 = layers.concatenate([flat_prev, flat_next, info])\n",
    "    x_1 = layers.Dense(816)(x_0)\n",
    "    x_2 = layers.Dense(816)(x_1)\n",
    "    x_3 = layers.Dense(816)(x_2)\n",
    "    x_4 = layers.Dense(408)(x_3)\n",
    "    x_5 = layers.Dense(204)(x_4)\n",
    "    x_6 = layers.Dense(204)(x_5)\n",
    "    x_7 = layers.Dense(102)(x_6)\n",
    "    x_8 = layers.Dense( 51)(x_7)\n",
    "    bold_signal = layers.Dense(1, activation=\"sigmoid\", name=\"bold_signal\")(x_8)\n",
    "\n",
    "    model = keras.Model(inputs=[prev_input, next_input, info_input], outputs=[bold_signal])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "          loss={\"bold_signal\": \"mean_squared_error\"},\n",
    "          loss_weights=[1.])\n",
    "#     if _checkpoint():\n",
    "#         model.load_weights(\"checkpoint_path\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pbezuhov/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "df                = pd.read_csv(\"/Volumes/hd_4tb/project/model_input.csv\")\n",
    "available_volumes = np.load(\"available_volumes.npy\")\n",
    "train_cols        = [c for c in df.columns if c.startswith(\"is_\")] + [\"age\"]\n",
    "model = _load_model()\n",
    "history_path = \"/Volumes/hd_4tb/results/history.csv\"\n",
    "checkpoint_path = \"/Volumes/hd_4tb/results/checkpoints\"\n",
    "batch_size  = 128\n",
    "num_epoches = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "For working with multiple inputs/outputs see [here](https://www.tensorflow.org/guide/keras/functional#models_with_multiple_inputs_and_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connhc conn001 s1 conscious\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc03a3bd3be488c9bc7574eb459451b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1280), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cartesian(data, timepoints):\n",
    "    ret = list()\n",
    "    for x, y, z in data:\n",
    "        for t in timepoints:\n",
    "            ret.append((x, y, z, t))\n",
    "    return ret\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(_get(row, \"project\"), _get(row, \"subject\"), _get(row, \"time_session\"), _get(row, \"task\"))\n",
    "    TR = 2 if _get(row, \"is_mb\") == 0 else 0.71\n",
    "    task = _get(row, \"task\")\n",
    "    save_path = join(checkpoint_path, \"weights%04d.h5\" % i)\n",
    "    if os.path.isfile(save_path):\n",
    "        continue\n",
    "    fmri_path = _fmri_path(row)\n",
    "    fmri      = _get_data(join(fmri_path, \"normalized.nii.gz\"))\n",
    "    grey      = _get_data(join(fmri_path, \"..\", \"structural\", \"gm_probseg.nii.gz\"))\n",
    "    onset_df  = pd.read_csv(join(fmri_path, \"onsets.csv\"))\n",
    "    info = pd.concat([pd.DataFrame(row[train_cols]).T] * batch_size).reset_index(drop=True)\n",
    "    \n",
    "    batch = {\"prev\": list(), \"next\": list()}\n",
    "    bold_signal, mask_rows = list(), list()\n",
    "    training_voxels = cartesian( available_volumes, np.array(range(1, fmri.shape[3]-1)) )\n",
    "    training_index = random.sample(range(len(training_voxels)), batch_size*10)\n",
    "\n",
    "    for j in tqdm(training_index):\n",
    "        x, y, z, t = training_voxels[j]\n",
    "        onsets = last_onset(onset_df, task, TR * t, max_time=1000)\n",
    "        onsets = pd.concat([onsets] * batch_size).reset_index(drop=True)\n",
    "        \n",
    "        mask_rows.append( _mask_info(masks, fmri, grey, (x, y, z, t))  )\n",
    "        grey_data = nii_input(grey, x, y, z)\n",
    "        for name, timepoint in [(\"prev\", t-1), (\"next\", t+1)]:\n",
    "            batch[name].append(\n",
    "                np.stack((_load_volume(fmri, x, y, z, timepoint), grey_data), axis=3)\n",
    "            )\n",
    "        bold_signal.append(fmri[x, y, z, t])\n",
    "        \n",
    "        ### Fit to data\n",
    "        if len(batch[\"prev\"]) == batch_size:\n",
    "            batch[\"info\"] = pd.concat([\n",
    "                onsets, info, pd.DataFrame(mask_rows)\n",
    "            ], axis=1)\n",
    "            batch[\"prev\"], batch[\"next\"] = np.array(batch[\"prev\"]), np.array(batch[\"next\"])\n",
    "            history = model.fit(\n",
    "                batch, {'bold_signal': np.array(bold_signal)},\n",
    "                epochs=num_epoches, batch_size=16\n",
    "            )\n",
    "            if os.path.isfile(history_path):\n",
    "                pd.concat([pd.read_csv(history_path), pd.DataFrame(history.history)]).to_csv(history_path, index=False)\n",
    "            else:\n",
    "                pd.DataFrame(history.history).to_csv(history_path, index=False)\n",
    "            batch = {\"prev\": list(), \"next\": list()}\n",
    "            bold_signal, mask_rows = list(), list()\n",
    "    model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
